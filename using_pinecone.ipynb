{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSB0pnt0gbOo"
      },
      "source": [
        "## üîÆ ‡§¨‡•ã‡§≤  ‡§≠‡§ø‡§°‡•Ç ! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "rACbepFGgbOo"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import streamlit as st\n",
        "\n",
        "co = cohere.Client(st.secrets.COHERE_API_KEY)\n",
        "# pc = Pinecone(api_key=st.secrets.PINECONE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Loading document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text has roughly 1030 words.\n"
          ]
        }
      ],
      "source": [
        "with open(\"C:/Users/mayur dabade/Desktop/Projects/marathi RAG/data/maharaj.txt\", encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"The text has roughly {len(text.split())} words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1aJ7hKGgbOr"
      },
      "source": [
        "### 2. Splitting doc into chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUph1JX41665",
        "outputId": "6c63a93f-6999-47af-e704-d4a88727bc75"
      },
      "outputs": [],
      "source": [
        "# For chunking let's use langchain to help us split the text\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhXW7iHC1-Q6",
        "outputId": "d68ac348-4b73-4c6a-a445-6c510bdb0881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text has been broken down in 15 chunks.\n"
          ]
        }
      ],
      "source": [
        "# Create basic configurations to chunk the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Split the text into chunks with some overlap\n",
        "chunks_ = text_splitter.create_documents([text])\n",
        "chunks = [c.page_content for c in chunks_]\n",
        "print(f\"The text has been broken down in {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8g0sE2hgbOs"
      },
      "source": [
        "### 3. Embed every text chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEarMPEqgbOs",
        "outputId": "7da0e06d-f637-4470-8e01-6de8249be64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We just computed 15 embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
        "model=\"embed-multilingual-v3.0\"\n",
        "response = co.embed(\n",
        "    texts= chunks,\n",
        "    model=model,\n",
        "    input_type=\"search_document\",\n",
        "    embedding_types=['float']\n",
        ")\n",
        "embeddings = response.embeddings.float\n",
        "print(f\"We just computed {len(embeddings)} embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Storing id, chunks and embeddings to pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# # Initialize Pinecone with your API key\n",
        "# api_key = os.getenv(\"PINECONE_API_KEY\", \"6727bdcd-c680-4056-9398-64bedaaee775\")\n",
        "# pc = Pinecone(api_key=api_key)\n",
        "\n",
        "# # Create a serverless index\n",
        "# # Ensure \"dimension\" matches the dimensions of the vectors you upsert\n",
        "# pc.create_index(name=\"products\", dimension=len(embeddings[0]), \n",
        "#                 spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
        "\n",
        "# # Target the index\n",
        "# index = pc.Index(\"products\")\n",
        "\n",
        "# # Prepare the vectors and metadata for upsert\n",
        "# vectors_to_upsert = []\n",
        "# for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "#     vector = {\n",
        "#         \"id\": f\"vector_{i}\",\n",
        "#         \"values\": embedding,\n",
        "#         \"metadata\": {\"description\": chunk}\n",
        "#     }\n",
        "#     vectors_to_upsert.append(vector)\n",
        "\n",
        "# # Upsert vectors into the Pinecone index\n",
        "# index.upsert(vectors=vectors_to_upsert)\n",
        "\n",
        "# print(\"Vectors successfully upserted into Pinecone!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Storing ids and embeddings to pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectors successfully upserted into Pinecone!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone with your API key\n",
        "pc = Pinecone(api_key=st.secrets.PINECONE_API_KEY)\n",
        "\n",
        "# Create a serverless index\n",
        "# Ensure \"dimension\" matches the dimensions of the vectors you upsert\n",
        "pc.create_index(name=\"product\", dimension=len(embeddings[0]), \n",
        "                spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
        "\n",
        "# Target the index\n",
        "index = pc.Index(\"product\")\n",
        "\n",
        "# Prepare the vectors for upsert (only id and embedding)\n",
        "vectors_to_upsert = []\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    vector = {\n",
        "        \"id\": f\"{i}\",\n",
        "        \"values\": embedding\n",
        "    }\n",
        "    vectors_to_upsert.append(vector)\n",
        "\n",
        "# Upsert vectors into the Pinecone index\n",
        "index.upsert(vectors=vectors_to_upsert)\n",
        "\n",
        "print(\"Vectors successfully upserted into Pinecone!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6NGVurZgbOs"
      },
      "source": [
        "### 5. Given a user query, retrieve the relevant chunks from the vector database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC05yJQ7jlek"
      },
      "source": [
        "### Define the user question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Y2HTxspKgbOs"
      },
      "outputs": [],
      "source": [
        "query = \"‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§ú‡§æ‡§∏‡•ç‡§§ ‡§∂‡•á‡§Ö‡§∞ ‡§Ö‡§∏‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§ï‡§Ç‡§™‡§®‡•Ä?\"\n",
        "# query=\"‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø‡§®‡•á ‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§≤‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§Ö‡§ü‡•Ä‡§Ç‡§∏‡§π ‡§Æ‡§Ç‡§ú‡•Å‡§∞‡•Ä ‡§ï‡§æ ‡§¶‡§ø‡§≤‡•Ä? ‡§Ø‡§æ ‡§Ö‡§ü‡•Ä‡§Ç‡§ö‡§æ ‡§â‡§¶‡•ç‡§¶‡•á‡§∂ ‡§ï‡§æ‡§Ø ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡•ã?\"\n",
        "# query = \"‡§Æ‡•Å‡§ò‡§≤ ‡§µ ‡§Ü‡§¶‡§ø‡§≤‡§∂‡§æ‡§π‡•Ä ‡§´‡•å‡§ú‡§æ‡§Ç‡§ö‡§æ ‡§Ø‡§∂‡§∏‡•ç‡§µ‡•Ä‡§™‡§£‡•á ‡§∏‡§æ‡§Æ‡§®‡§æ\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oULg1tOjjOW"
      },
      "source": [
        "### 6. Embed the user question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrUuS6vXgbOs",
        "outputId": "0c64a930-f817-43c2-d775-1d9145cb304e"
      },
      "outputs": [],
      "source": [
        "# Because the text being embedded is the search query, we set the input type as search_query\n",
        "response = co.embed(\n",
        "    texts=[query],\n",
        "    model=model,\n",
        "    input_type=\"search_query\",\n",
        "    embedding_types=['float']\n",
        ")\n",
        "query_embedding = response.embeddings.float[0]\n",
        "# print(\"query_embedding: \", query_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Retrieve the most relevant chunks from the vector database\n",
        "\n",
        "We use cosine similarity to find the most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'id': '4', 'score': 0.63326323}, {'id': '11', 'score': 0.611728072}, {'id': '1', 'score': 0.609839082}]\n"
          ]
        }
      ],
      "source": [
        "index = pc.Index(\"product\")\n",
        "\n",
        "query_results1 = index.query(\n",
        "    # namespace=\"example-namespace1\",\n",
        "    vector=query_embedding,\n",
        "    top_k=3,\n",
        "    include_values=True\n",
        ")\n",
        "# query_results1\n",
        "\n",
        "# Extracting id and score\n",
        "result = [{'id': match['id'], 'score': match['score']} for match in query_results1['matches']]\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4, 11, 1]\n",
            "\n",
            " ‡§Ø‡§æ ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§§ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§π‡•á ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§Ö‡§∏‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§°‡§ø‡§ú‡§®‡•Ä ‡§ï‡§° 3684% ‡§Ç‡§ö‡•Ä ‡§≠‡§æ‡§ó‡•Ä‡§¶‡§æ‡§∞‡•Ä ‡§Ö‡§∏‡•á‡§≤ ‡§§‡§∞ ‡§â‡§∞‡§≤‡•á‡§≤‡§æ 75% ‡§µ‡§æ‡§ü‡§æ ‡§π‡§æ ‡§∏‡•ç‡§ü‡§æ‡§∞ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ‡§ö‡•á ‡§∏‡•Ä‡§à‡§ì ‡§â‡§¶‡§Ø ‡§∂‡§Ç‡§ï‡§∞ ‡§Ü‡§£‡§ø ‡§ú‡•á‡§Æ‡•ç‡§∏ ‡§Æ‡•Å‡§∞‡§¶‡•ã‡§ú ‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§¨‡•ã‡§ß‡•Ä ‡§ü‡•ç‡§∞‡•Ä ‡§Ø‡§æ ‡§ú‡•â‡§à‡§Ç‡§ü ‡§µ‡•ç‡§π‡•á‡§Ç‡§ö‡§∞ ‡§ï‡§°‡•á ‡§Ö‡§∏‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ï‡•ã‡§∞‡•ç‡§ü‡§æ‡§¶‡•ç‡§µ‡§æ‡§∞‡•á ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§æ ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡§Ç‡§∏‡•ç‡§•‡•á‡§ö‡•Ä ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§Ö‡§∏‡§£‡§Ç ‡§ó‡§∞‡§ú‡•á‡§ö‡§Ç ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§®‡§Ç‡§§‡§∞‡§ö ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ø‡•á‡§ä ‡§∂‡§ï‡§§‡§æ‡§§ ‡§Ü‡§§‡§æ ‡§Ø‡§æ ‡§°‡•Ä‡§≤ ‡§∏‡§æ‡§†‡•Ä ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§ï‡•â‡§Æ‡•ç‡§™‡§ø‡§ü‡§ø‡§∂‡§® ‡§ï‡§Æ‡§ø‡§∂‡§® ‡§ë‡§´ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§ö‡•Ä ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§Ö‡§∏‡§£‡§Ç ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã‡§§‡§Ç ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§π‡•Ä ‡§µ‡•à‡§ß‡§æ‡§®‡§ø‡§ï ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡§æ‡§ú‡§æ‡§∞‡§™‡•á‡§†‡•á‡§§‡•Ä‡§≤ ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§ü‡§ø‡§ï‡§µ‡•Ç‡§® ‡§†‡•á‡§µ‡§£‡§Ç\n",
            "\n",
            " ‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§ú‡§æ‡§∏‡•ç‡§§ ‡§∂‡•á‡§Ö‡§∞ ‡§Ö‡§∏‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§°‡§ø‡§ú‡§®‡•Ä ‡§Ü‡§£‡§ø ‡§Ü‡§ï‡•ç‡§∞‡§Æ‡§ï‡§™‡§£‡•á ‡§Ø‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§§ ‡§Ü‡§ó‡•á‡§ï‡•Ç ‡§ï‡§∞‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§ö‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ø‡•á‡§£‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡§Ç ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§§ ‡§è‡§ï‡§æ ‡§ú‡§æ‡§Ø‡§Ç‡§ü ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§ö‡§æ ‡§ú‡§®‡•ç‡§Æ ‡§π‡•ã‡§£‡§Ç ‡§π‡•Ä ‡§Æ‡•ã‡§†‡•Ä ‡§ö‡§ø‡§Ç‡§§‡•á‡§ö‡•Ä ‡§¨‡§æ‡§¨ ‡§Ö‡§∏‡§≤‡•ç‡§Ø‡§æ‡§ö‡§Ç ‡§¨‡•ã‡§≤‡§≤‡§Ç ‡§ú‡§æ‡§§‡§Ç‡§Ø ‡§Ü‡§§‡§æ ‡§Ø‡•á‡§§‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§≥‡§æ‡§§ ‡§ú‡•á‡§µ‡•ç‡§π‡§æ ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§∏‡§æ‡§Æ‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§™‡•ç‡§∞‡§ï‡•ç‡§∑‡•á‡§™‡§£ ‡§ï‡•á‡§≤‡§Ç ‡§ú‡§æ‡§à‡§≤ ‡§§‡•á‡§µ‡•ç‡§π‡§æ ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü‡§ö‡•ç‡§Ø‡§æ ‡§¶‡§∞‡§Æ‡•ç‡§Ø‡§æ‡§® ‡§ú‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§π‡§ø‡§∞‡§æ‡§§‡•Ä ‡§¶‡§æ‡§ñ‡§µ‡§≤‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ‡§§ ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§¶‡§∞ ‡§π‡•á ‡§á‡§§‡§∞ ‡§ï‡•ã‡§£‡§§‡§æ‡§π‡•Ä ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§ï ‡§®‡§∏‡§≤‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§ñ‡•Ç‡§™ ‡§ú‡§æ‡§∏‡•ç‡§§ ‡§Ö‡§∏‡§§‡•Ä‡§≤ ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§Ø‡§æ ‡§ú‡§æ‡§π‡§ø‡§∞‡§æ‡§§ ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§¨‡§æ‡§∞‡•ç‡§ó‡•á‡§®‡§ø‡§Ç‡§ó ‡§™‡§æ‡§µ‡§∞ ‡§π‡•Ä ‡§ï‡§Æ‡•Ä ‡§π‡•ã‡§à‡§≤ ‡§Ø‡§æ‡§ö‡§æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§ú‡§æ‡§π‡§ø‡§∞‡§æ‡§§ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§µ‡§∞ ‡§π‡•ã‡§ä ‡§∂‡§ï‡§§‡•ã ‡§§‡§ø‡§•‡§≤‡•ç‡§Ø‡§æ ‡§õ‡•ã‡§ü‡•ç‡§Ø‡§æ\n",
            "\n",
            " ‡§á‡§Ç‡§°‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ú ‡§Ü‡§£‡§ø ‡§°‡§ø‡§ú‡§®‡•Ä ‡§∏‡•ç‡§ü‡§æ‡§∞ ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ï‡•â‡§Æ‡•ç‡§™‡§ø‡§ü‡§ø‡§∂‡§® ‡§ï‡§Æ‡§ø‡§∂‡§® ‡§ë‡§´ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§ï‡§°‡•Ç‡§® ‡§Æ‡§Ç‡§ú‡•Å‡§∞‡•Ä ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ø‡•á‡§£‡•ç‡§Ø‡§æ‡§®‡§Ç ‡§¶‡•á‡§∂‡§æ‡§§‡•Ä‡§≤ ‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§Æ‡•ã‡§†‡•ç‡§Ø‡§æ ‡§è‡§Ç‡§ü‡§∞‡§ü‡•á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§Ç‡§™‡§®‡•Ä‡§ö‡§æ ‡§ú‡§®‡•ç‡§Æ ‡§ù‡§æ‡§≤‡§æ‡§Ø ‡§ï‡§æ‡§∞‡§£ ‡§Ø‡§æ ‡§è‡§ï‡§Æ‡•á‡§µ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§ö‡§Ç ‡§Ü‡§§‡§æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§§‡§¨‡•ç‡§¨‡§≤ 34% ‡§ì‡§ü‡•Ä‡§ü‡•Ä ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á‡§ü‡§µ‡§∞ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§Ö‡§∏‡•á‡§≤ ‡§Ö‡§∏‡§Ç ‡§∏‡§æ‡§Ç‡§ó‡§ø‡§§‡§≤‡§Ç ‡§ú‡§æ‡§§‡§Ç‡§Ø ‡§∏‡§ß‡•ç‡§Ø‡§æ ‡§°‡§ø‡§ú‡§®‡•Ä ‡§Ü‡§£‡§ø ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§Ø‡§æ ‡§¶‡•ã‡§®‡•ç‡§π‡•Ä ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ ‡§è‡§ï‡§Æ‡•á‡§ï‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡•Ä ‡§Ü‡§π‡•á‡§§ ‡§™‡§£ ‡§Ü‡§§‡§æ ‡§Ø‡§æ ‡§¶‡•ã‡§®‡•ç‡§π‡•Ä ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ü‡§≤‡•ç‡§Ø‡§æ ‡§§‡§∞ ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ‡§∂‡•Ä ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§ï‡§∞‡§£‡§Ç ‡§á‡§§‡§∞ ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§Ö‡§µ‡§ò‡§° ‡§ú‡§æ‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§¶‡•ã‡§®‡•ç‡§π‡•Ä ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ø‡•á‡§ä‡§®\n"
          ]
        }
      ],
      "source": [
        "ids = [int(item['id']) for item in result]\n",
        "print(ids)\n",
        "\n",
        "for i in ids:\n",
        "    print(\"\\n\",chunks[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matching Chunks for the user query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = \"\\n\\n\".join(chunks[i] for i in ids)\n",
        "# print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. designing final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "template = f\"\"\"Use the following pieces of context to answer the user question. This context retrieved from a knowledge base and you should use only the facts from the context to answer.\n",
        "Your answer must be based on the context. If the context not contain the answer, just say that 'I don't know', don't try to make up an answer, use the context.\n",
        "Don't address the context directly, but use it to answer the user question like it's your own knowledge.\n",
        "Use three sentences maximum. answer should be in Marathi.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§ú‡§æ‡§∏‡•ç‡§§ ‡§∂‡•á‡§Ö‡§∞ ‡§Ö‡§∏‡§£‡§æ‡§∞‡•Ä ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§π‡•Ä ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§Ü‡§π‡•á.\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(st.secrets.GOOGLE_API_KEY)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(template)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a NumPy array\n",
        "arr = np.array(chunks)\n",
        "\n",
        "# Save the array to a .npy file\n",
        "np.save('my_array.npy', arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the array from the .npy file\n",
        "arr = np.load('my_array.npy')\n",
        "\n",
        "# Verify the loaded array\n",
        "print(len(arr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp4c_HkYIEn_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hackathon_docs_3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
