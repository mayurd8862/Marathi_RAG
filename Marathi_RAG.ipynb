{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSB0pnt0gbOo"
      },
      "source": [
        "## üîÆ ‡§¨‡•ã‡§≤  ‡§≠‡§ø‡§°‡•Ç ! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "rACbepFGgbOo"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import streamlit as st\n",
        "co = cohere.Client(st.secrets.COHERE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Loading document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text has roughly 1030 words.\n"
          ]
        }
      ],
      "source": [
        "with open(\"C:/Users/mayur dabade/Desktop/Projects/marathi RAG/data/maharaj.txt\", encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"The text has roughly {len(text.split())} words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1aJ7hKGgbOr"
      },
      "source": [
        "### 2. Splitting doc into chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUph1JX41665",
        "outputId": "6c63a93f-6999-47af-e704-d4a88727bc75"
      },
      "outputs": [],
      "source": [
        "# For chunking let's use langchain to help us split the text\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhXW7iHC1-Q6",
        "outputId": "d68ac348-4b73-4c6a-a445-6c510bdb0881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text has been broken down in 15 chunks.\n"
          ]
        }
      ],
      "source": [
        "# Create basic configurations to chunk the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Split the text into chunks with some overlap\n",
        "chunks_ = text_splitter.create_documents([text])\n",
        "chunks = [c.page_content for c in chunks_]\n",
        "print(f\"The text has been broken down in {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8g0sE2hgbOs"
      },
      "source": [
        "### 3. Embed every text chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEarMPEqgbOs",
        "outputId": "7da0e06d-f637-4470-8e01-6de8249be64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We just computed 15 embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
        "model=\"embed-multilingual-v3.0\"\n",
        "response = co.embed(\n",
        "    texts= chunks,\n",
        "    model=model,\n",
        "    input_type=\"search_document\",\n",
        "    embedding_types=['float']\n",
        ")\n",
        "embeddings = response.embeddings.float\n",
        "print(f\"We just computed {len(embeddings)} embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM6vKeypgbOs"
      },
      "source": [
        "### 4. Store the embeddings in a vector database\n",
        "\n",
        "We use the simplest vector database ever: a python dictionary using `np.array()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "H2srFH-IgbOs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "vector_database = {i: np.array(embedding) for i, embedding in enumerate(embeddings)}\n",
        "# { 0: array([...]), 1: array([...]), 2: array([...]), ..., 10: array([...]) }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6NGVurZgbOs"
      },
      "source": [
        "### 5. Given a user query, retrieve the relevant chunks from the vector database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC05yJQ7jlek"
      },
      "source": [
        "### Define the user question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Y2HTxspKgbOs"
      },
      "outputs": [],
      "source": [
        "# query = \"‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§ú‡§æ‡§∏‡•ç‡§§ ‡§∂‡•á‡§Ö‡§∞ ‡§Ö‡§∏‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§ï‡§Ç‡§™‡§®‡•Ä?\"\n",
        "# query = \"‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§á‡§Ç‡§°‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ú ‡§Ü‡§£‡§ø ‡§°‡§ø‡§ú‡§®‡•Ä ‡§∏‡•ç‡§ü‡§æ‡§∞ ‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§Æ‡•Å‡§≥‡•á ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§è‡§Ç‡§ü‡§∞‡§ü‡•á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§á‡§Ç‡§°‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ï‡•ã‡§£‡§§‡•á ‡§¨‡§¶‡§≤ ‡§π‡•ã‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ ‡§Ü‡§π‡•á?\"\n",
        "query=\"‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø‡§®‡•á ‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§≤‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§Ö‡§ü‡•Ä‡§Ç‡§∏‡§π ‡§Æ‡§Ç‡§ú‡•Å‡§∞‡•Ä ‡§ï‡§æ ‡§¶‡§ø‡§≤‡•Ä? ‡§Ø‡§æ ‡§Ö‡§ü‡•Ä‡§Ç‡§ö‡§æ ‡§â‡§¶‡•ç‡§¶‡•á‡§∂ ‡§ï‡§æ‡§Ø ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡•ã?\"\n",
        "# query = \"‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§Æ‡•Å‡§≥‡•á ‡§ú‡§æ‡§ó‡§§‡§ø‡§ï ‡§™‡§æ‡§§‡§≥‡•Ä‡§µ‡§∞ ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§Æ‡§æ ‡§ï‡§∂‡•Ä ‡§¨‡§¶‡§≤‡•Ç ‡§∂‡§ï‡§§‡•á?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oULg1tOjjOW"
      },
      "source": [
        "### 6. Embed the user question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrUuS6vXgbOs",
        "outputId": "0c64a930-f817-43c2-d775-1d9145cb304e"
      },
      "outputs": [],
      "source": [
        "# Because the text being embedded is the search query, we set the input type as search_query\n",
        "response = co.embed(\n",
        "    texts=[query],\n",
        "    model=model,\n",
        "    input_type=\"search_query\",\n",
        "    embedding_types=['float']\n",
        ")\n",
        "query_embedding = response.embeddings.float[0]\n",
        "# print(\"query_embedding: \", query_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K8B87CGgbOt"
      },
      "source": [
        "### 7. Retrieve the most relevant chunks from the vector database\n",
        "\n",
        "We use cosine similarity to find the most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nik3es32gbOt",
        "outputId": "a1c30024-52e1-42c7-8836-a2c590559aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similarity scores:  [0.39042216797176, 0.535859393001034, 0.5209220639794689, 0.5239729444354693, 0.580017305183357, 0.6314604602118782, 0.4650150473179502, 0.5710279982043929, 0.47029629840894555, 0.4687914650110989, 0.45571845249419357, 0.47513857655440933, 0.597099167130034, 0.541507375359911, 0.38312835166689446]\n",
            "Here are the indices of the top 5 chunks after retrieval:  [ 5 12  4  7 13]\n",
            "Here are the top 5 chunks after retrieval: \n",
            "\n",
            "\n",
            "== ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡§æ‡§ú‡§æ‡§∞‡§™‡•á‡§†‡•á‡§§‡•Ä‡§≤ ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§ü‡§ø‡§ï‡§µ‡•Ç‡§® ‡§†‡•á‡§µ‡§£‡§Ç ‡§§‡§ø‡§≤‡§æ ‡§™‡•ç‡§∞‡•ã‡§§‡•ç‡§∏‡§æ‡§π‡§® ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§ö‡§Ç ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§Ü‡§£‡§ø ‡§°‡§ø‡§ú‡§®‡•Ä‡§ö‡•ç‡§Ø‡§æ ‡§°‡•Ä‡§≤‡§Æ‡•Å‡§≥‡§Ç ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§è‡§Ç‡§ü‡§∞‡§ü‡•á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á‡§ü‡§Æ‡§ß‡§≤‡•Ä ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§®‡§∑‡•ç‡§ü ‡§π‡•ã‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§≠‡•Ä‡§§‡•Ä ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§ ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§§ ‡§π‡•ã‡§§‡•Ä ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§Ø‡§æ ‡§°‡•Ä‡§≤ ‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§ß‡•Ä ‡§§‡§Ø‡§æ‡§∞ ‡§®‡§µ‡•ç‡§π‡§§‡•Ä ‡§´‡•á‡§¨‡•ç‡§∞‡•Å‡§µ‡§æ‡§∞‡•Ä ‡§™‡§æ‡§∏‡•Ç‡§® ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§®‡•ç‡§Ø‡§æ ‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§≤‡§æ ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§∞‡§æ‡§ñ‡•Ç‡§® ‡§†‡•á‡§µ‡§≤‡§æ ‡§π‡•ã‡§§‡§æ ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§Ü‡§§‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§Ö‡§ü‡•Ä‡§Ç‡§∏‡§π 28 ‡§ë‡§ó‡§∏‡•ç‡§ü‡§≤‡§æ ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø‡§® ‡§Ø‡§æ ‡§°‡•Ä‡§≤‡§≤‡§æ ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§¶‡§ø‡§≤‡•Ä ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§≤‡§æ ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§® ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§®‡§Ç‡§§‡§∞ ‡§Ü‡§§‡§æ ‡§°‡§ø‡§ú‡§®‡•Ä‡§ö‡•á 80 ‡§Ü‡§£‡§ø ‡§µ‡§æ‡§Ø‡§ï‡•â‡§Æ 18 ‡§ö‡•á 40 ‡§Ö‡§∏‡•á ‡§è‡§ï‡•Ç‡§£ 120 ‡§ü‡•Ä‡§µ‡•ç‡§π‡•Ä ‡§ö‡•Ö‡§®‡§≤‡•ç‡§∏ ‡§§‡§Ø‡§æ‡§∞ ‡§π‡•ã‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§®‡§µ‡•Ä‡§®\n",
            "\n",
            "== ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§ú‡§æ‡§π‡§ø‡§∞‡§æ‡§§ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§µ‡§∞ ‡§π‡•ã‡§ä ‡§∂‡§ï‡§§‡•ã ‡§§‡§ø‡§•‡§≤‡•ç‡§Ø‡§æ ‡§õ‡•ã‡§ü‡•ç‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§π‡•Ä ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§¶‡•É‡§∑‡•ç‡§ü‡•ç‡§Ø‡§æ ‡§ù‡•á‡§™ ‡§®‡§∏‡•á‡§≤ ‡§Ö‡§∂‡•Ä ‡§≠‡•Ä‡§§‡•Ä ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§®‡•á ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§ ‡§ï‡•á‡§≤‡•Ä ‡§Ü‡§π‡•á ‡§§‡§∏‡•á‡§ö ‡§≠‡§æ‡§∞‡§§‡§æ‡§§ ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü‡§ö‡•á ‡§ï‡•ã‡§ü‡•ç‡§Ø‡§æ‡§µ‡§ß‡•Ä ‡§ö‡§æ‡§π‡§§‡•á ‡§Ü‡§π‡•á‡§§ ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§Ü‡§Ø‡§™‡•Ä‡§è‡§≤ ‡§Ü‡§£‡§ø ‡§á‡§§‡§∞ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡•Ö‡§ö‡•á‡§∏ ‡§™‡•ç‡§∞‡•á‡§ï‡•ç‡§∑‡§ï ‡§Ø‡§æ‡§ö ‡§™‡•ç‡§≤‡•Ö‡§ü‡§´‡•â‡§∞‡•ç‡§Æ‡§µ‡§∞ ‡§¨‡§ò‡•Ç ‡§∂‡§ï‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á‡§§ ‡§™‡•ç‡§∞‡•á‡§ï‡•ç‡§∑‡§ï‡§æ‡§Ç‡§∏‡§Æ‡•ã‡§∞ ‡§¶‡•Å‡§∏‡§∞‡§æ ‡§ï‡•ã‡§£‡§§‡§æ‡§π‡•Ä ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø ‡§∂‡§ø‡§≤‡•ç‡§≤‡§ï ‡§®‡§∏‡§≤‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§π‡•Ä ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§Ü‡§™‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§¨‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§®‡§ö‡•á ‡§¶‡§∞ ‡§µ‡§æ‡§¢‡§µ‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ‡§π‡•Ä ‡§Ü‡§π‡•á ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§Ü‡§£‡§ø ‡§°‡§ø‡§ú‡§®‡•Ä ‡§Æ‡§∞‡•ç‡§ú‡§∞ ‡§π‡•á ‡§è‡§ï‡§Ç‡§¶‡§∞‡•Ä‡§§ ‡§™‡§æ‡§π‡§§‡§æ ‡§Ü‡§µ‡•ç‡§π‡§æ‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§Ö‡§∏‡§≤‡•ç‡§Ø‡§æ‡§ö‡§Ç ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø‡§ö‡§Ç ‡§Æ‡§§ ‡§Ü‡§π‡•á ‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§Æ‡•Å‡§ï‡•á‡§∂ ‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä ‡§π‡•á ‡§Ü‡§§‡§æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§è‡§Ç‡§ü‡§∞‡§ü‡•á‡§®‡§Æ‡•á‡§Ç‡§ü\n",
            "\n",
            "== ‡§Ø‡§æ ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§§ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§π‡•á ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§Ö‡§∏‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§°‡§ø‡§ú‡§®‡•Ä ‡§ï‡§° 3684% ‡§Ç‡§ö‡•Ä ‡§≠‡§æ‡§ó‡•Ä‡§¶‡§æ‡§∞‡•Ä ‡§Ö‡§∏‡•á‡§≤ ‡§§‡§∞ ‡§â‡§∞‡§≤‡•á‡§≤‡§æ 75% ‡§µ‡§æ‡§ü‡§æ ‡§π‡§æ ‡§∏‡•ç‡§ü‡§æ‡§∞ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ‡§ö‡•á ‡§∏‡•Ä‡§à‡§ì ‡§â‡§¶‡§Ø ‡§∂‡§Ç‡§ï‡§∞ ‡§Ü‡§£‡§ø ‡§ú‡•á‡§Æ‡•ç‡§∏ ‡§Æ‡•Å‡§∞‡§¶‡•ã‡§ú ‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§¨‡•ã‡§ß‡•Ä ‡§ü‡•ç‡§∞‡•Ä ‡§Ø‡§æ ‡§ú‡•â‡§à‡§Ç‡§ü ‡§µ‡•ç‡§π‡•á‡§Ç‡§ö‡§∞ ‡§ï‡§°‡•á ‡§Ö‡§∏‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ï‡•ã‡§∞‡•ç‡§ü‡§æ‡§¶‡•ç‡§µ‡§æ‡§∞‡•á ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§æ ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡§Ç‡§∏‡•ç‡§•‡•á‡§ö‡•Ä ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§Ö‡§∏‡§£‡§Ç ‡§ó‡§∞‡§ú‡•á‡§ö‡§Ç ‡§Ü‡§π‡•á ‡§Ø‡§æ ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§®‡§Ç‡§§‡§∞‡§ö ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ø‡•á‡§ä ‡§∂‡§ï‡§§‡§æ‡§§ ‡§Ü‡§§‡§æ ‡§Ø‡§æ ‡§°‡•Ä‡§≤ ‡§∏‡§æ‡§†‡•Ä ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§ï‡•â‡§Æ‡•ç‡§™‡§ø‡§ü‡§ø‡§∂‡§® ‡§ï‡§Æ‡§ø‡§∂‡§® ‡§ë‡§´ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§ö‡•Ä ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§Ö‡§∏‡§£‡§Ç ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã‡§§‡§Ç ‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø ‡§π‡•Ä ‡§µ‡•à‡§ß‡§æ‡§®‡§ø‡§ï ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡§æ‡§ú‡§æ‡§∞‡§™‡•á‡§†‡•á‡§§‡•Ä‡§≤ ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§ü‡§ø‡§ï‡§µ‡•Ç‡§® ‡§†‡•á‡§µ‡§£‡§Ç\n",
            "\n",
            "== ‡§Ü‡§π‡•á‡§§ ‡§Ü‡§§‡§æ ‡§Ü‡§§‡§æ ‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§Æ‡•Å‡§≥‡§Ç ‡§§‡•ç‡§Ø‡§æ‡§§ ‡§°‡§ø‡§ú‡•ç‡§®‡•Ä‡§ö‡•Ä ‡§π‡•Ä ‡§≠‡§∞ ‡§™‡§°‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§§‡§∏‡§Ç‡§ö ‡§°‡§ø‡§ú‡•ç‡§®‡•Ä‡§ö‡•á 30 ‡§π‡§ú‡§æ‡§∞‡§æ‡§π‡•Ç‡§® ‡§Ö‡§ß‡§ø‡§ï ‡§ï‡§Ç‡§ü‡•á‡§Ç‡§ü ‡•≤‡§∏‡•á‡§ü‡•ç‡§∏ ‡§µ‡§æ‡§™‡§∞‡§£‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§π‡§ï‡•ç‡§ï ‡§Ü‡§£‡§ø ‡§°‡§ø‡§ú‡§®‡•Ä‡§ö‡•ç‡§Ø‡§æ ‡§∏‡§ø‡§®‡•á‡§Æ‡§æ‡§Ç‡§ö‡•á ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§µ‡§ø‡§§‡§∞‡§£‡§æ‡§ö‡•á ‡§π‡§ï‡•ç‡§ï‡§π‡•Ä ‡§Ø‡§æ ‡§®‡§µ‡•Ä‡§® ‡§ï‡§Ç‡§™‡§®‡•Ä‡§≤‡§æ ‡§Æ‡§ø‡§≥‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á‡§§ ‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡§Ç ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§™‡•Å‡§¢‡§ö‡•Ä ‡§ï‡§æ‡§π‡•Ä ‡§µ‡§∞‡•ç‡§∑ ‡§§‡§∞‡•Ä ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§ï‡§°‡§Ç ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§™‡•ç‡§∞‡§ï‡•ç‡§∑‡•á‡§™‡§£‡§æ‡§ö‡•á ‡§è‡§ï ‡§π‡§æ‡§§‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§Ö‡§∏‡§§‡•Ä‡§≤ ‡§Ø‡§æ‡§ö‡§Ç ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§Ü‡§Ø‡§™‡•Ä‡§è‡§≤ ‡§ö‡•ç‡§Ø‡§æ ‡§ì‡§ü‡•Ä‡§ü‡•Ä ‡§™‡•ç‡§∞‡§ï‡•ç‡§∑‡•á‡§™‡§£‡§æ‡§ö‡•á ‡§π‡§ï‡•ç‡§ï ‡§π‡•á ‡§µ‡•â‡§Ø‡§ï‡•â‡§Æ ‡§ï‡§°‡•á ‡§§‡§∞ 2027 ‡§™‡§∞‡•ç‡§Ø‡§Ç‡§§ ‡§Ü‡§Ø‡§∏‡•Ä‡§∏‡•Ä ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü‡§ö‡•ç‡§Ø‡§æ ‡§ü‡•Ä‡§µ‡•ç‡§π‡•Ä ‡§™‡•ç‡§∞‡§ï‡•ç‡§∑‡•á‡§™‡§£‡§æ‡§ö‡•á ‡§π‡§ï‡•ç‡§ï ‡§π‡•á ‡§°‡§ø‡§ú‡§®‡•Ä ‡§ï‡§°‡•á ‡§Ü‡§π‡•á‡§§ ‡§§‡§∏‡•á‡§ö ‡§µ‡•Å‡§Æ‡§®‡•ç‡§∏ ‡§™‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ø‡§∞ ‡§≤‡•Ä‡§ó ‡§π‡•á 2027 ‡§™‡§∞‡•ç‡§Ø‡§Ç‡§§‡§ö‡•á ‡§ó‡•ç‡§≤‡•ã‡§¨‡§≤ ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ\n",
            "\n",
            "== ‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§Æ‡•Å‡§ï‡•á‡§∂ ‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä ‡§π‡•á ‡§Ü‡§§‡§æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§è‡§Ç‡§ü‡§∞‡§ü‡•á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§á‡§Ç‡§°‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ö‡•á ‡§ú‡§æ‡§Ø‡§Ç‡§ü ‡§ï‡§ø‡§Ç‡§ó ‡§†‡§∞‡§≤‡•ç‡§Ø‡§æ‡§ö‡§Ç ‡§¨‡•ã‡§≤‡§≤‡§Ç ‡§ú‡§æ‡§§‡§Ç‡§Ø ‡§Ø‡§æ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§≤‡§æ ‡§ï‡•ã‡§∞‡•ç‡§ü‡§æ‡§ö‡•Ä ‡§™‡§∞‡§µ‡§æ‡§®‡§ó‡•Ä ‡§Ö‡§∏‡§£‡§Ç ‡§ó‡§∞‡§ú‡•á‡§ö‡§Ç ‡§Ö‡§∏‡§≤‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§ï‡•ã‡§∞‡•ç‡§ü ‡§Ø‡§æ ‡§ó‡•ã‡§∑‡•ç‡§ü‡•Ä‡§Ç‡§ö‡§æ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡§∞‡§£‡§æ‡§∞ ‡§ï‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞ ‡§∏‡§æ‡§†‡•Ä ‡§Ø‡§æ ‡§¶‡•ã‡§®‡•ç‡§π‡•Ä ‡§ï‡§Ç‡§™‡§®‡•ç‡§Ø‡§æ‡§Ç‡§∏‡§Æ‡•ã‡§∞ ‡§®‡•á‡§Æ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡•Å‡§†‡§≤‡•ç‡§Ø‡§æ ‡§Ö‡§ü‡•Ä ‡§†‡•á‡§µ‡§£‡§æ‡§∞ ‡§π‡•á ‡§™‡§æ‡§π‡§£‡§Ç ‡§Ü‡§§‡§æ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡§Ç ‡§Ö‡§∏‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§™‡§£ ‡§è‡§ï‡§Ç‡§¶‡§∞‡•Ä‡§§‡§ö ‡§Ø‡§æ ‡§ï‡§Ç‡§™‡§®‡•Ä‡§§ ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§ö‡§æ ‡§µ‡§æ‡§ü‡§æ ‡§π‡§æ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï ‡§Ö‡§∏‡§≤‡•ç‡§Ø‡§æ‡§®‡§Ç ‡§Æ‡•Å‡§ï‡•á‡§∂ ‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä ‡§Ü‡§§‡§æ ‡§è‡§Ç‡§ü‡§∞‡§ü‡•á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§á‡§Ç‡§°‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ö‡•á‡§π‡•Ä ‡§è‡§ï‡§ü‡•á‡§ö ‡§¨‡•â‡§∏ ‡§ù‡§æ‡§≤‡•ç‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§π‡•ã‡§§ ‡§Ü‡§π‡•á‡§§ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡§æ‡§Ø ‡§µ‡§æ‡§ü‡§§‡§Ç ‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§Æ‡•Å‡§≥‡•á ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§§ ‡§∞‡§ø‡§≤‡§æ‡§Ø‡§®‡•ç‡§∏ ‡§ö‡•Ä ‡§Æ‡•ã‡§®‡•ã‡§™‡•ã‡§≤‡•Ä ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§π‡•ã‡§£‡§æ‡§∞ ‡§ï‡§æ ‡§Ü‡§£‡§ø ‡§§‡§∏‡§Ç\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Calculate similarity between the user question & each chunk\n",
        "similarities = [cosine_similarity(query_embedding, chunk) for chunk in embeddings]\n",
        "print(\"similarity scores: \", similarities)\n",
        "\n",
        "# Get indices of the top 10 most similar chunks\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Keep only the top 10 indices\n",
        "top_indices = sorted_indices[:5]\n",
        "print(\"Here are the indices of the top 5 chunks after retrieval: \", top_indices)\n",
        "\n",
        "# Retrieve the top 10 most similar chunks\n",
        "top_chunks_after_retrieval = [chunks[i] for i in top_indices]\n",
        "print(\"Here are the top 5 chunks after retrieval: \\n\")\n",
        "for t in top_chunks_after_retrieval:\n",
        "    print(\"\\n== \" + t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. designing final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the context for the template\n",
        "context = \"\\n\".join(top_chunks_after_retrieval)\n",
        "\n",
        "# Template for the answer\n",
        "template = f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. answer should be in marathi\n",
        " \n",
        "{context}\n",
        "Question: {query}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "# print(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡§Ø‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§Æ‡§ú‡§ï‡•Ç‡§∞‡§æ‡§§ ‡§¶‡§ø‡§≤‡•á‡§≤‡•á ‡§®‡§æ‡§π‡•Ä.\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(st.secrets.GOOGLE_API_KEY)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(template)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡§∏‡•Ä‡§∏‡•Ä‡§Ü‡§Ø‡§®‡•á ‡§Ø‡§æ ‡§Æ‡§∞‡•ç‡§ú‡§∞‡§≤‡§æ 28 ‡§ë‡§ó‡§∏‡•ç‡§ü‡§≤‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§Ö‡§ü‡•Ä‡§Ç‡§∏‡§π ‡§Æ‡§Ç‡§ú‡•Å‡§∞‡•Ä ‡§¶‡§ø‡§≤‡•Ä ‡§Ü‡§π‡•á. ‡§π‡•ç‡§Ø‡§æ ‡§Ö‡§ü‡•Ä‡§Ç‡§ö‡§æ ‡§â‡§¶‡•ç‡§¶‡•á‡§∂ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§∏‡•ç‡§™‡§∞‡•ç‡§ß‡§æ ‡§ü‡§ø‡§ï‡§µ‡•Ç‡§® ‡§†‡•á‡§µ‡§£‡§Ç, ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü‡§ö‡•á ‡§ï‡•ã‡§ü‡•ç‡§Ø‡§æ‡§µ‡§ß‡•Ä ‡§ö‡§æ‡§π‡§§‡•á ‡§Ü‡§£‡§ø ‡§™‡•ç‡§∞‡•á‡§ï‡•ç‡§∑‡§ï‡§æ‡§Ç‡§∏‡§Æ‡•ã‡§∞ ‡§¶‡•Å‡§∏‡§∞‡§æ ‡§ï‡•ã‡§£‡§§‡§æ‡§π‡•Ä ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø ‡§∂‡§ø‡§≤‡•ç‡§≤‡§ï ‡§®‡§∏‡§≤‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§π‡•Ä ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§Ü‡§™‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§¨‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§®‡§ö‡•á ‡§¶‡§∞ ‡§µ‡§æ‡§¢‡§µ‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ‡§π‡•Ä ‡§Ü‡§π‡•á.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat = ChatGroq(temperature=0, groq_api_key=st.secrets.GROQ_API_KEY, model_name=\"llama3-8b-8192\")\n",
        "\n",
        "\n",
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | chat\n",
        "a = chain.invoke({\"text\": template})\n",
        "print(a.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp4c_HkYIEn_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hackathon_docs_3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
